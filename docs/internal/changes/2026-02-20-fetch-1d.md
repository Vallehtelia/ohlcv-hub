# 2026-02-20: Fetch 1d End-to-End Implementation

## What Changed

- **Implemented full fetch 1d workflow**: Fetch → Normalize → Validate → Export
- **Added normalization module**: Converts raw Alpaca bars to stable DataFrame schema
- **Added validation module**: Validates data quality and detects missing trading days
- **Added export module**: Exports DataFrame to Parquet/CSV formats
- **Added dataset orchestration**: Coordinates fetch, normalize, and validate steps
- **Updated CLI**: Wired up full workflow for daily bars, weekly still unimplemented

## Files Created

### Core Modules
- `ohlcv_hub/normalize.py` - `bars_dict_to_dataframe()` function
- `ohlcv_hub/validate.py` - `validate_daily_bars()` function
- `ohlcv_hub/export.py` - `export_dataframe()` function
- `ohlcv_hub/dataset.py` - `build_daily_dataset()` orchestration function

### Tests
- `tests/test_fetch_1d_integration_mocked.py` - End-to-end integration tests with mocked API

### Documentation
- `docs/changes/2026-02-20-fetch-1d.md` - This file
- `docs/llm-handoff/feature3-plan-2026-02-20.md` - Implementation plan

## Files Modified

- `ohlcv_hub/cli.py` - Added `_make_alpaca_client()` and full fetch 1d implementation
- `README.md` - Updated usage examples and output description

## Key Decisions / Tradeoffs

### 1. Date Extraction for Missing Days
- **Decision**: Convert timestamps to NY timezone (`America/New_York`), then extract date
- **Rationale**: Alpaca daily bars use 04:00Z (midnight ET), need NY date for trading calendar comparison
- **Implementation**: `ts.dt.tz_convert('America/New_York').dt.date`
- **Tradeoff**: Requires timezone conversion, but ensures correct trading day mapping

### 2. Export-on-Validation-Error Policy
- **Decision**: Export data even if validation errors exist, but exit with code 1
- **Rationale**: MVP safety - don't lose data, but signal issues to user
- **Behavior**: Print error summary, export data, exit code 1
- **Tradeoff**: May export invalid data, but user can inspect and decide

### 3. Missing Days as Warnings
- **Decision**: Missing trading days don't fail the run (warnings only)
- **Rationale**: Data gaps are informational, not errors (holidays, delistings, etc.)
- **Behavior**: Included in report, but don't affect exit code
- **Tradeoff**: May mask real data issues, but aligns with expected behavior

### 4. Single File Output
- **Decision**: One file per run (not partitioned by symbol or date)
- **Rationale**: Simpler MVP, easier to handle
- **Filename**: `ohlcv_1d_<YYYYMMDD>_<YYYYMMDD>.parquet`
- **Tradeoff**: Less efficient for large datasets, but can add partitioning later

### 5. Sorting in Normalize Step
- **Decision**: Sort DataFrame by symbol, then ts (ascending) during normalization
- **Rationale**: Required for monotonic timestamp validation, deterministic output
- **Tradeoff**: Adds processing time, but ensures consistent output

### 6. Test Injection Point
- **Decision**: Add `_make_alpaca_client(config)` function for test injection
- **Rationale**: Enables CLI integration tests without real network calls
- **Implementation**: Monkeypatch in tests to inject MockTransport client
- **Tradeoff**: Slight indirection, but essential for testability

### 7. Feed Selection
- **Decision**: Use `feed="iex"` for MVP (hardcoded in CLI)
- **Rationale**: IEX feed doesn't require subscription, reduces API access issues
- **Tradeoff**: Less comprehensive data, but more accessible for testing

## Implementation Details

### Normalize Module (`ohlcv_hub/normalize.py`)

**Function**: `bars_dict_to_dataframe()`

**Input**: Raw bars dict from Alpaca API
- Format: `{symbol: [{t, o, h, l, c, v, n?, vw?}, ...]}`

**Output**: pandas DataFrame with schema:
- `symbol`: string
- `timeframe`: string ("1d")
- `ts`: datetime64[ns, UTC]
- `open`, `high`, `low`, `close`: float64
- `volume`: int64
- `source`: string ("alpaca")
- `currency`: string (default "USD")
- `adjustment`: string ("raw" or "all")

**Behavior**:
- Parses RFC-3339 timestamps to UTC datetime
- Sorts by symbol, then ts (ascending)
- Handles missing optional fields (n, vw)

### Validate Module (`ohlcv_hub/validate.py`)

**Function**: `validate_daily_bars()`

**Validations**:
1. **Duplicates**: Check for duplicate (symbol, ts) pairs
2. **Monotonic timestamps**: Ensure strictly increasing ts within each symbol
3. **OHLC sanity**:
   - `high >= max(open, close)`
   - `low <= min(open, close)`
   - `high >= low`
4. **Volume**: Check for negative volume
5. **Missing trading days**: Compare against NYSE calendar

**Report Structure**:
```json
{
  "summary": {
    "symbols_count": int,
    "bars_count": int,
    "date_range": {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
  },
  "issues": {
    "duplicates": [{"symbol": str, "ts": str, "count": int}],
    "non_monotonic": [{"symbol": str, "example_ts_prev": str, "example_ts_next": str}],
    "ohlc_violations": {"count": int, "samples": [...]},
    "volume_violations": {"count": int, "samples": [...]}
  },
  "missing_days": {
    "SYMBOL": ["YYYY-MM-DD", ...],
    "totals": {"missing_days_count_total": int}
  }
}
```

**Return**: `(ok: bool, report_dict: dict)`
- `ok` is True only if no duplicates, no monotonic issues, no OHLC violations, no negative volume
- Missing days don't affect `ok` status

### Export Module (`ohlcv_hub/export.py`)

**Function**: `export_dataframe()`

**Behavior**:
- Creates output directory if it doesn't exist
- Writes Parquet via `df.to_parquet(engine="pyarrow", index=False)`
- Writes CSV via `df.to_csv(index=False)`
- Returns full file path

### Dataset Module (`ohlcv_hub/dataset.py`)

**Function**: `build_daily_dataset()`

**Orchestration**:
1. Call `client.fetch_stock_bars()` with `timeframe="1Day"`
2. Normalize via `bars_dict_to_dataframe()`
3. Validate via `validate_daily_bars()`
4. Return `(df, report_dict)`

### CLI Modifications (`ohlcv_hub/cli.py`)

**New Function**: `_make_alpaca_client(config) -> AlpacaClient`
- Enables test injection via monkeypatching

**Fetch Command Changes**:
- If `tf == 1w`: Keep old behavior (exit 2, "not implemented")
- If `tf == 1d`:
  1. Load config
  2. Create AlpacaClient
  3. Call `build_daily_dataset()`
  4. Export via `export_dataframe()`
  5. Write `validation_report.json` if `--report`
  6. Print summary
  7. Exit 0 on success, 1 on validation errors (but data still exported)

**Exit Codes**:
- `0`: Success (export completed, report written if enabled)
- `1`: Validation/provider errors (data still exported)
- `2`: Unimplemented (weekly path)

## Test Coverage

### Integration Tests (`tests/test_fetch_1d_integration_mocked.py`)

1. **test_fetch_1d_writes_parquet_and_report**:
   - Mocks 1-page bars response (2 symbols, next_page_token=None)
   - Runs CLI fetch command
   - Asserts exit code 0
   - Asserts parquet file exists and has correct columns
   - Asserts validation_report.json exists and has correct structure

2. **test_fetch_1d_handles_provider_error**:
   - Mocks 401 response
   - Asserts exit code 1
   - Asserts error message mentions provider error

3. **test_fetch_1w_still_exits_2**:
   - Asserts weekly path still exits with code 2

**Testing Strategy**:
- Use `httpx.MockTransport` to mock API responses
- Monkeypatch `ohlcv_hub.cli._make_alpaca_client` to inject MockTransport client
- No real network calls in tests

## How to Test

### Automated Tests

```bash
# Run all tests
pytest -q

# Run integration tests
pytest tests/test_fetch_1d_integration_mocked.py -v

# Run with coverage
pytest --cov=ohlcv_hub --cov-report=html
```

### Manual Smoke Test

```bash
# Set environment variables
export ALPACA_API_KEY=your_key
export ALPACA_API_SECRET=your_secret

# Fetch daily bars
ohlcv-hub fetch \
  --symbols SPY,QQQ \
  --start 2024-01-01 \
  --end 2024-02-01 \
  --tf 1d \
  --out ./data \
  --format parquet \
  --report

# Verify output files exist
ls -lh ./data/
# Should see:
# - ohlcv_1d_20240101_20240201.parquet
# - validation_report.json

# Verify parquet contents
python -c "import pandas as pd; df = pd.read_parquet('./data/ohlcv_1d_20240101_20240201.parquet'); print(df.head()); print(f'Rows: {len(df)}'); print(f'Symbols: {df[\"symbol\"].unique()}')"

# Verify validation report
cat ./data/validation_report.json | jq .
```

## TODOs / Follow-ups

### Immediate Next Steps
1. **Weekly Resampling**: Implement `--tf 1w` path
   - Resample daily bars to weekly (Monday 00:00 UTC)
   - Aggregate OHLCV correctly
   - Apply same validation and export

2. **Feed Selection**: Add `--feed` CLI option
   - Allow user to choose `iex`, `sip`, etc.
   - Default to `iex` for MVP

3. **Error Handling Improvements**:
   - Better error messages for validation failures
   - Option to skip export on validation errors (flag)

### Future Enhancements
1. **Partitioning Strategy**: Partition output by symbol or date range
   - More efficient for large datasets
   - Easier to query specific symbols

2. **Richer Validation Reports**:
   - Add statistics (min/max prices, volume trends)
   - Add data quality scores
   - Add visualization options

3. **Caching**: Add optional caching for API responses
   - Reduce API calls for repeated requests
   - Cache key: symbols + date range + adjustment

4. **Retry Logic**: Add retry for transient errors
   - Handle 429 rate limits
   - Handle 500 server errors
   - Exponential backoff

5. **Progress Indicators**: Add progress bars for long-running fetches
   - Show pagination progress
   - Show validation progress

6. **CSV Export Options**: Add CSV formatting options
   - Date format options
   - Decimal precision
   - Separator options

7. **Data Validation Enhancements**:
   - Price change validation (detect suspicious jumps)
   - Volume spike detection
   - Cross-symbol consistency checks

8. **Corporate Actions**: Integrate corporate actions endpoint
   - Map splits/dividends to bars
   - Validate adjustment correctness

## Notes

- **Schema Compliance**: Output schema matches mini-spec exactly
- **Timezone Handling**: All timestamps stored as UTC, converted to NY for trading day detection
- **Missing Days Detection**: Uses `pandas_market_calendars` NYSE calendar
- **Export Policy**: Data exported even with validation errors (exit code 1 signals issues)
- **Weekly Path**: Still exits with code 2, not yet implemented
- **Test Coverage**: All tests use MockTransport, no real network calls

## Verification Checklist

- [x] Fetch 1d works end-to-end
- [x] Writes parquet/csv file with correct schema
- [x] Writes validation_report.json when `--report`
- [x] Weekly path still exits 2
- [x] All tests pass (no real network calls)
- [x] Schema matches mini-spec exactly
- [x] Missing days detection works
- [x] Validation catches duplicates, monotonic issues, OHLC violations
- [x] Export-on-validation-error policy implemented
- [x] Test injection point works
